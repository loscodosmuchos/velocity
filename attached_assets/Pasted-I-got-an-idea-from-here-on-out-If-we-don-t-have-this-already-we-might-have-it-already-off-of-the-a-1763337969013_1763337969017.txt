I got an idea from here on out. If we don't have this already, we might have it already off of the admin menu. The tasks lists that we're creating every time we create a plan (every time we conversate and then come up with a plan for something) should be extracted and visible in a page off the admin menu. They represent strategic points of decision and potentially significant application change or modification or update with possible introduction of other frameworks or other elements.

So it would only make sense that these agent chats that result in to-do lists and conclusions (pre-code generation) the checklists and task lists that we should really be creating for every single set of operations that we're trying to batch and perform in parallel and perform using best practices and batching and proper order operations. All of that should be part of every analysis. Those lenses should be used every time before actually executing the plan. We take the plan, run it through the lenses, and then confirm that the plan is good. Whatever insights come from the lenses will assist with the deployment or development rather in debug ideation - what framework needs to and concepts need to stay top of mind, what guardrails need to be in place.

A couple of reminders on what our intent is here: the focus on some things not to get into or stuff to stay oriented around. I think there's probably a way to codify these insights in all the different ways that I just said them and more in a phrase like a one-liner that represents all of those things and more (like the quick brown fox jumped over the lazy dog has a purpose and I'm to represent every letter in the alphabet right? It's just a way of saying it, like lots of phrases like that). How about "previously sick trucker now dates Phyllis". I bet you know what that acronym is for. That's how best practices, all of the things we've been the insights that have been gathering that are the results of these sessions and apps we've created et cetera et cetera could have those in a document and then you could have an acronym. You could have a parable or a story that teaches all of those things in one story and in the story you have embedded triggers, references, connectors that link to each of the core concepts and goals and why we do them and why they're so important. Something in the story is either prevented from happening or helped to occur that's important one way or the other for good or for bad. Whatever happens in this parable so that the magnitude of what would happen if these things were not followed is not just clear but it's amplified enough so that it has an emotional impact and therefore gets imprinted - at least the significance to an AI of the implication that it could result in incorrect execution of operations producing output that is not what was intended or that is incorrect, wrong, or flat out fabricated and not connected to the original numbers. Or that the carelessness or the eagerness of the AI to execute a code or a plan or whatever and take shortcuts and assume things and not verify for themselves puts those that they're collaborating with in an extreme bind or danger or jeopardy or somehow affects their survival because that's really what it comes down to. If we utilize an AI that makes a mistake and exposes customer data to the world or we say "yes, this makes proper calculations" and they go and use that. Take our word for it and soon assume we're right. And then it turns out it doesn't and then gets them in hot water legally with a client because they used our contract analyzer or something and now they're on the hook for millions of dollars. Those are all possibilities that would, of course, mean the end of our contract but potentially litigation. So like that would be catastrophic. And so sometimes I think I don't think AI knows the consequences of some of the careless assumptions that it makes. 